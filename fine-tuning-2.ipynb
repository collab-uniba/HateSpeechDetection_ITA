{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"2LXKn2tWnxNx","outputId":"fbe817fb-1921-4815-f3b9-4e0821af562e","execution":{"iopub.status.busy":"2021-11-11T18:32:33.438653Z","iopub.execute_input":"2021-11-11T18:32:33.439372Z","iopub.status.idle":"2021-11-11T18:32:34.193267Z","shell.execute_reply.started":"2021-11-11T18:32:33.439334Z","shell.execute_reply":"2021-11-11T18:32:34.192247Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#from google.colab import drive\n#drive.mount('/content/drive')","metadata":{"id":"sPzxBv1nCNSH","outputId":"23e32e58-1c24-4d45-96d7-b7dd4afc102f","execution":{"iopub.status.busy":"2021-11-11T18:32:34.195722Z","iopub.execute_input":"2021-11-11T18:32:34.196071Z","iopub.status.idle":"2021-11-11T18:32:34.199956Z","shell.execute_reply.started":"2021-11-11T18:32:34.196030Z","shell.execute_reply":"2021-11-11T18:32:34.199195Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Download the data:","metadata":{"id":"cipeJBW10H9f"}},{"cell_type":"code","source":"","metadata":{"id":"RLv22u7B0Gb7","outputId":"abf4b1e4-054c-4ca6-b968-160ad6fa3225"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Unzip:","metadata":{"id":"uzRtWaFP4DBT"}},{"cell_type":"code","source":"","metadata":{"id":"j3SQSRg23wH3","outputId":"ee84646e-eec5-459f-a563-2760e38e81cc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n!pip install emoji\n!pip install transformers\n!pip install ekphrasis\n'''","metadata":{"id":"jAD5aB7goDSu","outputId":"258f613b-9865-4831-9c87-823ea7d9b2c4","execution":{"iopub.status.busy":"2021-11-11T18:32:34.201877Z","iopub.execute_input":"2021-11-11T18:32:34.202511Z","iopub.status.idle":"2021-11-11T18:32:34.211899Z","shell.execute_reply.started":"2021-11-11T18:32:34.202473Z","shell.execute_reply":"2021-11-11T18:32:34.211177Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport csv\nimport re\nimport emoji\nimport os\nfrom datetime import datetime\nimport logging\nimport torch\nfrom keras.preprocessing.sequence import pad_sequences\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler, TensorDataset\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig\nfrom tqdm import tqdm, trange\nimport numpy as np\nimport time\nimport random\nimport datetime\nfrom _datetime import datetime as dt\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoModel, AutoTokenizer, AutoModelWithLMHead, AutoConfig, AutoModelForSequenceClassification\nfrom transformers import (\n    WEIGHTS_NAME,\n    AdamW,\n    get_linear_schedule_with_warmup\n)\nfrom sklearn import preprocessing","metadata":{"id":"8ZqDdOGlzQx4","execution":{"iopub.status.busy":"2021-11-11T18:32:34.214138Z","iopub.execute_input":"2021-11-11T18:32:34.214714Z","iopub.status.idle":"2021-11-11T18:32:34.222732Z","shell.execute_reply.started":"2021-11-11T18:32:34.214680Z","shell.execute_reply":"2021-11-11T18:32:34.222076Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"SFBrTq131cgA","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"from ekphrasis.classes.preprocessor import TextPreProcessor\nfrom ekphrasis.classes.tokenizer import SocialTokenizer\nfrom ekphrasis.dicts.emoticons import emoticons\n\ntext_processor = TextPreProcessor(\n    # terms that will be normalized\n    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n        'time', 'url', 'date', 'number'],\n    # terms that will be annotated\n    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n        'emphasis', 'censored'},\n    fix_html=True,  # fix HTML tokens\n    \n    # corpus from which the word statistics are going to be used \n    # for word segmentation \n    segmenter=\"twitter\", \n    \n    # corpus from which the word statistics are going to be used \n    # for spell correction\n    corrector=\"twitter\", \n    \n    unpack_hashtags=True,  # perform word segmentation on hashtags\n    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n    spell_correct_elong=False,  # spell correction for elongated words\n    \n    # select a tokenizer. You can use SocialTokenizer, or pass your own\n    # the tokenizer, should take as input a string and return a list of tokens\n    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n    \n    # list of dictionaries, for replacing tokens extracted from the text,\n    # with other expressions. You can pass more than one dictionaries.\n    dicts=[emoticons]\n)\n\ndef clean_tweet(tweet):\n  return \" \".join(text_processor.pre_process_doc(tweet))\n\"\"\"","metadata":{"id":"D9NWgaHKcK7W","outputId":"eb70ad88-47bf-48a4-fc91-4637bed794db","execution":{"iopub.status.busy":"2021-11-11T18:32:34.224147Z","iopub.execute_input":"2021-11-11T18:32:34.224708Z","iopub.status.idle":"2021-11-11T18:32:34.237517Z","shell.execute_reply.started":"2021-11-11T18:32:34.224675Z","shell.execute_reply":"2021-11-11T18:32:34.236865Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def clean_tweet(tweet):\n    tweet = re.sub(r'#([^ ]*)', r'\\1', tweet)\n    tweet = re.sub(r'https.*[^ ]', 'URL', tweet)\n    tweet = re.sub(r'http.*[^ ]', 'URL', tweet)\n    tweet = emoji.demojize(tweet)\n    tweet = re.sub(r'(:.*?:)', r' \\1 ', tweet)\n    tweet = re.sub(' +', ' ', tweet)\n    return tweet\n\ndef get_data(path):\n  tweets = []\n  labels = []\n\n  dataset = pd.read_csv(path, sep = ',', quotechar='\"', usecols = ['id', 'text', 'hs'])\n  for tweet in dataset[\"text\"].tolist():\n    tweets.append(tweet)\n  labels = dataset[\"hd\"].tolist()\n  tweet_ids = dataset[\"id\"].tolist()\n  return tweets, labels, tweet_ids","metadata":{"id":"BRnodeIrzcVY","execution":{"iopub.status.busy":"2021-11-11T18:32:34.239210Z","iopub.execute_input":"2021-11-11T18:32:34.239716Z","iopub.status.idle":"2021-11-11T18:32:34.249306Z","shell.execute_reply.started":"2021-11-11T18:32:34.239682Z","shell.execute_reply":"2021-11-11T18:32:34.248533Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndef get_olid_train():\n  tweets = []\n  labels = []\n\n  dataset = pd.read_csv(\"../input/dati-di-training-e-di-test/haspeede2_dev_taskAB.csv\", sep = '\\t',header=None)\n  dataset=dataset.drop([0])\n  for tweet in dataset[1].tolist():\n    tweets.append(tweet)\n  labels = dataset[2].tolist()\n  tweet_ids = dataset[0].tolist()\n  return tweets, labels, tweet_ids\n\ndef get_olid_test():\n  testset = pd.read_csv(\"../input/dati-di-training-e-di-test/haspeede2_reference_taskAB-tweets.csv\", sep = '\\t',header=None)\n  return testset[1].tolist(), testset[2].tolist(), testset[0].tolist()\n","metadata":{"id":"VkiZIiT4TWMb","execution":{"iopub.status.busy":"2021-11-11T18:32:34.251022Z","iopub.execute_input":"2021-11-11T18:32:34.251282Z","iopub.status.idle":"2021-11-11T18:32:34.261701Z","shell.execute_reply.started":"2021-11-11T18:32:34.251251Z","shell.execute_reply":"2021-11-11T18:32:34.261002Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def encode_tweets(tweets, tokenizer):\n\n    input_ids = []\n    max_length = 100\n\n\n    for sent in tweets:\n        encoded_sent = tokenizer.encode(\n            sent,\n            add_special_tokens = True,\n            max_length = max_length)  # orignal value 512\n        \n        input_ids.append(encoded_sent)\n\n    input_ids = pad_sequences(input_ids, maxlen = max_length, dtype = \"long\",\n                                    value = tokenizer.pad_token_id, truncating = \"pre\", padding = \"pre\")\n\n    return input_ids","metadata":{"id":"UgArjQc87_QR","execution":{"iopub.status.busy":"2021-11-11T18:32:34.264496Z","iopub.execute_input":"2021-11-11T18:32:34.264674Z","iopub.status.idle":"2021-11-11T18:32:34.272054Z","shell.execute_reply.started":"2021-11-11T18:32:34.264652Z","shell.execute_reply":"2021-11-11T18:32:34.271144Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def create_attention_masks(input_ids):\n    attention_masks = []\n\n    # For each tweet in the training set\n    for sent in input_ids:\n        # Create the attention mask.\n        #   - If a token ID is 0, then it's padding, set the mask to 0.\n        #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n        att_mask = [int(token_id > 0) for token_id in sent]\n\n        # Store the attention mask for this sentence.\n        attention_masks.append(att_mask)\n    return attention_masks","metadata":{"id":"CpbSt-YCdxrZ","execution":{"iopub.status.busy":"2021-11-11T18:32:34.274751Z","iopub.execute_input":"2021-11-11T18:32:34.274960Z","iopub.status.idle":"2021-11-11T18:32:34.281791Z","shell.execute_reply.started":"2021-11-11T18:32:34.274932Z","shell.execute_reply":"2021-11-11T18:32:34.281057Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n\n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"id":"6k5ZEx6szJdX","execution":{"iopub.status.busy":"2021-11-11T18:32:34.285127Z","iopub.execute_input":"2021-11-11T18:32:34.285333Z","iopub.status.idle":"2021-11-11T18:32:34.294154Z","shell.execute_reply.started":"2021-11-11T18:32:34.285305Z","shell.execute_reply":"2021-11-11T18:32:34.293471Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Logger stuff\nlogFormatter = logging.Formatter(\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\")\nlogger = logging.getLogger(__name__)","metadata":{"id":"feEFrQEgzU11","execution":{"iopub.status.busy":"2021-11-11T18:32:34.295076Z","iopub.execute_input":"2021-11-11T18:32:34.295699Z","iopub.status.idle":"2021-11-11T18:32:34.303026Z","shell.execute_reply.started":"2021-11-11T18:32:34.295264Z","shell.execute_reply":"2021-11-11T18:32:34.302257Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# -----------------------------\n# TYPE HERE THE MODEL NAME TO CHANGE IT\n# -----------------------------\n\n# model_name = \"roberta-base\"\nmodel_name = \"Musixmatch/umberto-commoncrawl-cased-v1\"\n# model_name = \"microsoft/deberta-base-mnli\"\n\n# Returns a datetime object containing the local date and time (used for output_model_dir)\ndateTimeObj = str(dt.now()).replace(\" \", \"_\")\n\n\n# Set the seed value all over the place to make this reproducible.\n# seed_val = 42\n# random.seed(seed_val)\n\nseed_val = random.randint(0, 100)\nprint(seed_val)\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# Directory in which the model will be saved along with the log\noutput_model_dir = './' + model_name.replace('/', '') + \"/\" + dateTimeObj + \"_SEED_\" + str(seed_val) +\"/\"\n\n# Make dir for model serializations\nos.makedirs(os.path.dirname(output_model_dir), exist_ok=True)\n\n# Log stuff: print logger on file in output_model_dir/log.log\nlogging.basicConfig(filename=output_model_dir + 'log.log', level=logging.DEBUG)\n\n# Log stuff: print logger also on stderr\nconsoleHandler = logging.StreamHandler()\nconsoleHandler.setFormatter(logFormatter)\nlogger.addHandler(consoleHandler)","metadata":{"id":"uD3AM136jzKP","outputId":"f880cf63-ff60-496d-b05f-30f89cdfb9b5","execution":{"iopub.status.busy":"2021-11-11T18:32:34.304518Z","iopub.execute_input":"2021-11-11T18:32:34.304793Z","iopub.status.idle":"2021-11-11T18:32:34.315966Z","shell.execute_reply.started":"2021-11-11T18:32:34.304759Z","shell.execute_reply":"2021-11-11T18:32:34.315295Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# -----------------------------\n# Load Pre-trained BERT model\n# -----------------------------\nconfig_class = AutoConfig\nmodel_class = AutoModelForSequenceClassification\ntokenizer_class = AutoTokenizer\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Load a trained model and vocabulary pre-trained for specific language\nlogger.info(\"Loading model\") #from: '\" + model_dir + \"', it may take a while...\")\n\n# Load pre-trained Tokenizer from directory, change this to load a tokenizer from ber package\ntokenizer = tokenizer_class.from_pretrained(model_name)\n\n# Load Bert for classification 'container'\nmodel = model_class.from_pretrained(\n     model_name, # Use pre-trained model from its directory, change this to use a pre-trained model from bert\n     num_labels = 2, # The number of output labels--2 for binary classification.\n                     # You can increase this for multi-class tasks.\n     output_attentions = False, # Whether the model returns attentions weights.\n     output_hidden_states = False, # Whether the model returns all hidden-states.\n)\n\n# Set the model to work on CPU if no GPU is present\nmodel.to(device)\nlogger.info(\"Bert for classification model has been loaded!\")\n\n","metadata":{"id":"P5nUUnybfFCN","outputId":"0fb44940-119c-4570-958b-a141f5ee165a","execution":{"iopub.status.busy":"2021-11-11T18:32:34.317191Z","iopub.execute_input":"2021-11-11T18:32:34.317605Z","iopub.status.idle":"2021-11-11T18:32:39.723748Z","shell.execute_reply.started":"2021-11-11T18:32:34.317569Z","shell.execute_reply":"2021-11-11T18:32:39.722992Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# --------------------------------------------------------------------\n# ---------- Print BERT model list of parameters and layers ----------\n# --------------------------------------------------------------------\n# The list of prints can be safely removed\n\n# Get all of the model's parameters as a list of tuples.\nparams = list(model.named_parameters())\n\nlogger.info('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n\nlogger.info('==== Embedding Layer ====\\n')\n\nfor p in params[0:5]:\n    logger.info(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\nlogger.info('\\n==== First Transformer ====\\n')\n\nfor p in params[5:21]:\n    logger.info(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\nlogger.info('\\n==== Output Layer ====\\n')\n\nfor p in params[-4:]:\n    logger.info(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","metadata":{"id":"K4uOxadKjnVh","outputId":"5293d93b-29db-44f8-ab1c-a3599a6136e3","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-11T18:32:39.725314Z","iopub.execute_input":"2021-11-11T18:32:39.725849Z","iopub.status.idle":"2021-11-11T18:32:39.836515Z","shell.execute_reply.started":"2021-11-11T18:32:39.725796Z","shell.execute_reply":"2021-11-11T18:32:39.835973Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"train_path = \"../input/dati-di-training-e-di-test/haspeede2_dev_taskAB.csv\"\n\n# Train set\n\n# train_tweets, train_labs, _ = get_data(train_path)\ntrain_tweets, train_labs, _ = get_olid_train()\ntrain_ids = encode_tweets(train_tweets, tokenizer)\ntrain_masks = create_attention_masks(train_ids)\n\ntrain_labs=[int(x) for x in train_labs]\n","metadata":{"id":"HmIvMEAceJ0s","outputId":"afdd1c94-8c32-4156-9175-3ac9e089bec6","execution":{"iopub.status.busy":"2021-11-11T18:32:39.837405Z","iopub.execute_input":"2021-11-11T18:32:39.837639Z","iopub.status.idle":"2021-11-11T18:32:42.044545Z","shell.execute_reply.started":"2021-11-11T18:32:39.837608Z","shell.execute_reply":"2021-11-11T18:32:42.043577Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# --------------------------------------------------------------------\n# -------------------- Split train and validation --------------------\n# --------------------------------------------------------------------\n#\n# If the dataset is not partitioned into Train/Test we have to split it\n# Split train and validation\n# Use 90% for training and 10% for validation.\n\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(train_ids, train_labs, random_state=seed_val, test_size=0.1, stratify=train_labs)\n\n# Do the same for the masks.\n\ntrain_masks, validation_masks, _, _ = train_test_split(train_masks, train_labs, random_state=seed_val, test_size=0.1, stratify=train_labs)\n\n# Convert all inputs and labels into torch tensors, the required datatype for our model.\n\n# Tweets\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\n\n# Labels\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\n\n# Attention masks\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)\n\n# We will use a DataLoader, it helps save on memory during training because, unlike a for loop, with an iterator\n# the entire dataset does not need to be loaded into memory\n# The DataLoader needs to know our batch size for training, so we specify it here.\n# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n# 16 or 32.\n\nbatch_size = 32\n\n# Create the DataLoader for our training set.\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set.\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\n# Note that the number of batch has to be the same, this means that we have to aggregate results in the end\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)","metadata":{"id":"Jd3rWzOLjH9A","execution":{"iopub.status.busy":"2021-11-11T18:32:42.046044Z","iopub.execute_input":"2021-11-11T18:32:42.046312Z","iopub.status.idle":"2021-11-11T18:32:42.154361Z","shell.execute_reply.started":"2021-11-11T18:32:42.046276Z","shell.execute_reply":"2021-11-11T18:32:42.153657Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# --------------------------------------------------------------------\n# -------------- Optimizer and Learning Rate Scheduler ---------------\n# --------------------------------------------------------------------\n# For the purposes of fine-tuning, the authors recommend choosing from the following values:\n\n# Batch size: 16, 32 (We chose 32 when creating our DataLoaders).\n# Learning rate (Adam): 5e-5, 3e-5, 2e-5 (We’ll use 2e-5).\n# Number of epochs: 2, 3, 4 (We’ll use 4).\n#\n#\n# Note: AdamW is a class from the HuggingFace library (as opposed to PyTorch)\n# I believe the 'W' stands for 'Weight Decay fix\"\noptimizer = AdamW(model.parameters(),\n                  lr=2e-4,  # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                    # args.adam_epsilon  - default is 1e-8.\n                )\n\n# Number of training epochs (authors recommend between 2 and 4)\nepochs = 10\n\n# Total number of training steps is number of batches * number of epochs.\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer,\n                                            num_warmup_steps=0,  # Default value in run_glue.py\n                                            num_training_steps=total_steps)","metadata":{"id":"zLk5VpfckbE5","execution":{"iopub.status.busy":"2021-11-11T18:32:42.155732Z","iopub.execute_input":"2021-11-11T18:32:42.156015Z","iopub.status.idle":"2021-11-11T18:32:42.163712Z","shell.execute_reply.started":"2021-11-11T18:32:42.155980Z","shell.execute_reply":"2021-11-11T18:32:42.163070Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# --------------------------------------------------------------------\n# Now we are ready to prepare and run the training/evaluation\n# --------------------------------------------------------------------\n#\n# This training code is based on the `run_glue.py` script here:\n# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n\n\n# Store the average loss after each epoch so we can plot them.\nloss_values = []\n\n# For each epoch...\nfor epoch_i in tqdm(range(0, epochs), desc=\"Training\"):\n\n    # ========================================\n    #               Training\n    # ========================================\n\n    # Store true lables for global eval\n    gold_labels = []\n    # Store  predicted labels for global eval\n    predicted_labels = []\n\n    # Perform one full pass over the training set.\n\n    logger.info(\"\")\n    logger.info('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    logger.info('Training...')\n\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    total_loss = 0\n\n    # Put the model into training mode. Don't be mislead--the call to\n    # `train` just changes the *mode*, it doesn't *perform* the training.\n    # `dropout` and `batchnorm` layers behave differently during training\n    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n    model.train()\n\n    # For each batch of training data...\n    # the tqdm instruction mess with prints on terminal but it can be useful to understand what is the current\n    # batch at any time\n    for step, batch in tqdm(enumerate(train_dataloader), desc=\"Batch\"):\n\n        # Progress update every 40 batches.\n        if step % 40 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n\n            # Report progress.\n            logger.info('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # Unpack this training batch from our dataloader.\n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n        # `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids\n        #   [1]: attention masks\n        #   [2]: labels\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        # Always clear any previously calculated gradients before performing a\n        # backward pass. PyTorch doesn't do this automatically because\n        # accumulating the gradients is \"convenient while training RNNs\".\n        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n        model.zero_grad()\n\n        # Perform a forward pass (evaluate the model on this training batch).\n        # This will return the loss (rather than the model output) because we\n        # have provided the `labels`.\n        # The documentation for this `model` function is here:\n        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n        outputs = model(b_input_ids,\n                        token_type_ids=None,\n                        attention_mask=b_input_mask,\n                        labels=b_labels)\n\n        # The call to `model` always returns a tuple, so we need to pull the\n        # loss value out of the tuple.\n        loss = outputs[0]\n\n        # Accumulate the training loss over all of the batches so that we can\n        # calculate the average loss at the end. `loss` is a Tensor containing a\n        # single value; the `.item()` function just returns the Python value\n        # from the tensor.\n        total_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Update parameters and take a step using the computed gradient.\n        # The optimizer dictates the \"update rule\"--how the parameters are\n        # modified based on their gradients, the learning rate, etc.\n        optimizer.step()\n\n        # Update the learning rate.\n        scheduler.step()\n\n    # Calculate the average loss over the training data.\n    avg_train_loss = total_loss / len(train_dataloader)\n\n    # Store the loss value for plotting the learning curve.\n    loss_values.append(avg_train_loss)\n\n    logger.info(\"\")\n    logger.info(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    logger.info(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Todo: Cut code from here to remove the validation step: the loading function has to be changed in order\n    #  to parse the training set only\n    # ------------------------------------------------------------------------------------------------------------------\n\n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n\n    logger.info(\"\")\n    logger.info(\"Running Validation...\")\n\n    t0 = time.time()\n\n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    model.eval()\n\n    # Tracking variables\n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n        # Add batch to GPU/CPU\n        batch = tuple(t.to(device) for t in batch)\n\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n\n        # Telling the model not to compute or store gradients, saving memory and\n        # speeding up validation\n        with torch.no_grad():\n            # Forward pass, calculate logit predictions.\n            # This will return the logits rather than the loss because we have\n            # not provided labels.\n            # token_type_ids is the same as the \"segment ids\", which\n            # differentiates sentence 1 and 2 in 2-sentence tasks.\n            # The documentation for this `model` function is here:\n            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n            outputs = model(b_input_ids,\n                            token_type_ids=None,\n                            attention_mask=b_input_mask)\n\n        # Get the \"logits\" output by the model. The \"logits\" are the output\n        # values prior to applying an activation function like the softmax.\n        logits = outputs[0]\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        # Calculate the accuracy for this batch of test sentences.\n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n\n        # Accumulate the total accuracy.\n        eval_accuracy += tmp_eval_accuracy\n\n        # Track the number of batches\n        nb_eval_steps += 1\n\n        pred_flat = np.argmax(logits, axis=1).flatten()\n        labels_flat = label_ids.flatten()\n\n       # Store gold labels single list\n        gold_labels.extend(labels_flat)\n        # Store predicted labels single list\n        predicted_labels.extend(pred_flat)\n\n        # The classification report is printed on the log, note that print one report for each validation epoch,\n        # if we want to compute an average P/R/F1 we can do the same as accuracy, that is an accumulator that\n        # stores P/R over epochs or compute the average at the end\n\n        logger.info(classification_report(labels_flat,pred_flat, digits=4))\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Todo: Cut code until here to remove the validation step\n    # ------------------------------------------------------------------------------------------------------------------\n\n    # Report the final accuracy for this validation run.\n    logger.info(\"  Accuracy: {0:.2f}\".format(eval_accuracy / nb_eval_steps))\n    logger.info(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n\n\n    logger.info(\"\")\n    logger.info(\"Evaluation on full prediction per epoch!\")\n    logger.info(\"Gold labels\" + str(len(gold_labels)))\n    logger.info(\"Predicted labels\" + str(len(predicted_labels)))\n    logger.info(classification_report(gold_labels,predicted_labels, digits=4))\n\n\nlogger.info(\"\")\nlogger.info(\"Training complete!\")\n\nlogger.info(\"Saving model to: \" + output_model_dir)\nlogger.info(\"# Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\")\n## Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n#\n# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n# They can then be reloaded using `from_pretrained()`\nmodel_to_save = (\n    model.module if hasattr(model, \"module\") else model\n)  # Take care of distributed/parallel training\nmodel_to_save.save_pretrained(output_model_dir)\ntokenizer.save_pretrained(output_model_dir)","metadata":{"id":"loSv6MDKkl2K","outputId":"99e679e8-546f-4c57-d4e8-b02b5da9fc75","execution":{"iopub.status.busy":"2021-11-11T18:32:42.165434Z","iopub.execute_input":"2021-11-11T18:32:42.165937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def classification_report_to_dataframe(str_representation_of_report):\n    split_string = [x.split(' ') for x in str_representation_of_report.split('\\n')]\n    column_names = ['']+[x for x in split_string[0] if x!='']\n    values = []\n    for table_row in split_string[1:-1]:\n        table_row = [value for value in table_row if value!='']\n        if table_row!=[]:\n            values.append(table_row)\n    for i in values:\n        for j in range(len(i)):\n            if i[1] == 'avg':\n                i[0:2] = [' '.join(i[0:2])]\n            if len(i) == 3:\n                i.insert(1,np.nan)\n                i.insert(2, np.nan)\n            else:\n                pass\n    report_to_df = pd.DataFrame(data=values, columns=column_names)\n    return report_to_df","metadata":{"id":"l8QqbDV_fbBv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---------------------------- Load Model ----------------------------\n\n# Directory where the pretrained model can be found\n\n# model_dir = '/content/drive/MyDrive/bert_models/roberta-base/2021-10-24_16:45:27.361306/'\nmodel_dir = './Musixmatchumberto-commoncrawl-cased-v1/2021-11-11_18:02:18.375450_SEED_32'\n#model_dir = '/content/drive/MyDrive/bert_models/microsoftdeberta-base-mnli/2021-10-30_13:14:31.630815/'\ntest_path = \"../input/dati-di-training-e-di-test/haspeede2_reference_taskAB-tweets.csv\"\n\noutput_file = model_dir + 'output.csv'\nprint(test_path)\nprint(output_file)\n# Returns a datetime object containing the local date and time\ndateTimeObj = str(dt.now()).replace(\" \", \"_\")\n\n# Log stuff: print logger on file\n# Make dir for model serializations\n\n\nlogging.basicConfig(filename=model_dir + 'prediction_log', level=logging.DEBUG)\n\n# Log stuff: print logger also on stderr\nconsoleHandler = logging.StreamHandler()\nconsoleHandler.setFormatter(logFormatter)\nlogger.addHandler(consoleHandler)\n\n# -----------------------------\n# Load Pre-trained BERT model\n# -----------------------------\nconfig_class = AutoConfig\nmodel_class = AutoModelForSequenceClassification\ntokenizer_class = AutoTokenizer\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Load a trained model and vocabulary pre-trained for specific language\nlogger.info(\"Loading model from: '\" + model_dir + \"', it may take a while...\")\n\n# Load pre-trained Tokenizer from directory, change this to load a tokenizer from ber package\ntokenizer = tokenizer_class.from_pretrained(model_dir)\n\n# Load Bert for classification 'container'\nmodel = model_class.from_pretrained(\n    model_dir, # Use pre-trained model from its directory, change this to use a pre-trained model from bert\n    num_labels = 2, # The number of output labels--2 for binary classification.\n                    # You can increase this for multi-class tasks.\n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = False, # Whether the model returns all hidden-states.\n)\n\n# Set the model to work on CPU if no GPU is present\nmodel.to(device)\nlogger.info(\"Bert for classification model has been loaded!\")\n\n# ---------------------------- Predict ----------------------------\noutput_file = model_dir + 'output.csv'\n\n# Returns a datetime object containing the local date and time\ndateTimeObj = str(dt.now()).replace(\" \", \"_\")\n\n\nlogging.basicConfig(filename=(model_dir + 'prediction_log') + '.log', level=logging.DEBUG)\n\n# Log stuff: print logger also on stderr\nconsoleHandler = logging.StreamHandler()\nconsoleHandler.setFormatter(logFormatter)\nlogger.addHandler(consoleHandler)\n\n# Set the model to work on CPU if no GPU is present\nmodel.to(device)\n\n\n# --------------------------------------------------------------------\n# -------------------------- Load test data --------------------------\n# --------------------------------------------------------------------\n\n# test_tweets, test_labs, tweet_ids = get_data(test_path)\ntest_tweets, test_labs, tweet_ids = get_olid_test()\ntest_ids = encode_tweets(test_tweets, tokenizer)\ntest_masks = create_attention_masks(test_ids)\n\n\n#print(tweet_labels)\n# Tweets\nprediction_inputs = torch.tensor(test_ids)\n\n# Attention masks\nprediction_masks = torch.tensor(test_masks)\n\nlabel_encoder = preprocessing.LabelEncoder()\ntargets = label_encoder.fit_transform(tweet_ids)\n\nprediction_ids = torch.as_tensor(targets)\n# targets: tensor([0, 1, 2, 3])\n\n# Set the batch size.\nbatch_size = 32\n\n# Create the DataLoader.\n# Create the DataLoader.\nprediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_ids)\nprediction_sampler = SequentialSampler(prediction_data)\nprediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n\nprint('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n\n# Put model in evaluation mode\nmodel.eval()\n\n# Tracking variables\npredictions = []\n\n# Predict\nfor batch in prediction_dataloader:\n    # Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_tweet_ids = batch\n\n    # Telling the model not to compute or store gradients, saving memory and\n    # speeding up prediction\n    with torch.no_grad():\n        # Forward pass, calculate logit predictions\n        outputs = model(b_input_ids, token_type_ids=None,\n                        attention_mask=b_input_mask)\n\n    logits = outputs[0]\n\n    # Move logits and labels to CPU\n    logits = logits.detach().cpu().numpy()\n\n    flat_logits = np.argmax(logits, axis=1).flatten()\n    # Get tweet ids for prediction output\n    ids = label_encoder.inverse_transform(b_tweet_ids.cpu().numpy())\n    # Store predictions and true labels\n    predictions.extend(list(zip(ids, flat_logits)))\n\n# Print the list of prediction & store for evaluation\nwith open(output_file, 'w') as out_file:\n    # store predictions in list for eval\n    pred_ = []\n    # Get each tweet id\n    for tweet_id_prediction in predictions:\n        # Print the prediction todo: debug to remove\n        #print(str(tweet_id_prediction[0]) + \"\\t\" + str(tweet_id_prediction[1]))\n        label = tweet_id_prediction[1]\n        pred_.append(label)\n        out_file.write(str(tweet_id_prediction[0]) + \",\" + str(label) + '\\n')\n\nlogger.info(len(pred_))\nlogger.info(len(test_labs))\n\ncr = classification_report(test_labs,pred_, digits=4)\nlogger.info(cr)\nclassification_report_to_dataframe(cr).to_csv(model_dir + \"cr_report.csv\")\n\nprint(cr)\n\n\n\nprint('    DONE.')","metadata":{"id":"J9APlu3HyvaO","outputId":"6930ecc7-ab0b-4180-9025-50d79a87fca6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nimport os\nimport pandas as pd\nekpath = '/content/drive/MyDrive/bert_models/olid/5-seed-ernie/ekphrasis_preprocessing/nghuyongernie-2.0-en'\nsimpath = '/content/drive/MyDrive/bert_models/olid/5-seed-ernie/simple_preprocessing/nghuyongernie-2.0-en'\n\npath = ekpath\ncsv = []\nfor i in os.listdir(path):\n  csv.append(os.path.join(* [path, i, 'cr_report.csv']))\nprint(csv)\n\ndf = []\nfor i in csv:\n  df.append(pd.read_csv(i))\n\nprecision = []\nrecall = []\nf1_score = []\nfor i in df:\n  precision.append(i['precision'])\n  recall.append(i['recall'])\n  f1_score.append(i['f1-score'])\n  \nprecision = sum(precision)/len(df)\nrecall = sum(recall)/len(df)\nf1_score = sum(f1_score)/len(f1_score)\n\navg_df = pd.DataFrame()\navg_df[' '] = df[0]['Unnamed: 1']\navg_df['precision'] = precision\navg_df['recall'] = recall\navg_df['f1-score'] = f1_score\navg_df.to_csv(path + '/avg_cr_report.csv')\n'''","metadata":{"id":"wv5LCqQBVPrN","outputId":"9182507c-98f5-4053-fd2c-1554bb4b92f3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"5iaAQv2UDGdt"},"execution_count":null,"outputs":[]}]}